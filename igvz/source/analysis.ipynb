{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGVZ Intervention Study - Effect on Migrant Women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>groupID</th>\n",
       "      <th>Patient_ID</th>\n",
       "      <th>IGVZ_intervention</th>\n",
       "      <th>Years_of_education</th>\n",
       "      <th>Country_of_origin</th>\n",
       "      <th>Gravida</th>\n",
       "      <th>Para</th>\n",
       "      <th>T1_Gestational_age</th>\n",
       "      <th>T1_KQ_1</th>\n",
       "      <th>T1_KQ_2</th>\n",
       "      <th>...</th>\n",
       "      <th>T2_KQ_13a_Score</th>\n",
       "      <th>T2_KQ_13b_Score</th>\n",
       "      <th>T2_KQ_13c_Score</th>\n",
       "      <th>T2_KQ_13d_Score</th>\n",
       "      <th>T2_KQ_13e_Score</th>\n",
       "      <th>T2_KQ_13f_Score</th>\n",
       "      <th>T2_KQ_13g_Score</th>\n",
       "      <th>T2_KQ_13h_Score</th>\n",
       "      <th>T2_KQ_13i_Score</th>\n",
       "      <th>T2_KQ_Total_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2018001</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2018002</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2018003</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2018004</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2018005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0</td>\n",
       "      <td>2020081</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0</td>\n",
       "      <td>2020083</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>99</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0</td>\n",
       "      <td>2020087</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "      <td>2020090</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>99</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0</td>\n",
       "      <td>2020092</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows Ã— 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     groupID  Patient_ID  IGVZ_intervention  Years_of_education  \\\n",
       "0          1     2018001                  1                  99   \n",
       "1          1     2018002                  1                  99   \n",
       "2          1     2018003                  1                  99   \n",
       "3          1     2018004                  1                  12   \n",
       "4          1     2018005                  1                  12   \n",
       "..       ...         ...                ...                 ...   \n",
       "125        0     2020081                  0                  12   \n",
       "126        0     2020083                  0                   0   \n",
       "127        0     2020087                  0                  10   \n",
       "128        0     2020090                  0                  18   \n",
       "129        0     2020092                  0                   8   \n",
       "\n",
       "     Country_of_origin  Gravida  Para  T1_Gestational_age  T1_KQ_1  T1_KQ_2  \\\n",
       "0                    1        2     1                37.0        2        1   \n",
       "1                    2        1     0                28.0        2        3   \n",
       "2                    3        1     0                99.0        2        3   \n",
       "3                    4        1     0                38.0        3        1   \n",
       "4                    3        1     0                35.0        1        3   \n",
       "..                 ...      ...   ...                 ...      ...      ...   \n",
       "125                  3        1     0                27.0        2        3   \n",
       "126                  3        5    99                30.0        3        1   \n",
       "127                  3        2    99                32.0        1        3   \n",
       "128                  3        5    99                20.0        2        1   \n",
       "129                 13        1     0                33.0        3        3   \n",
       "\n",
       "     ...  T2_KQ_13a_Score  T2_KQ_13b_Score  T2_KQ_13c_Score  T2_KQ_13d_Score  \\\n",
       "0    ...                0                0                0                0   \n",
       "1    ...                1                1                0                1   \n",
       "2    ...                0                0                0                0   \n",
       "3    ...                0                0                0                0   \n",
       "4    ...                0                0                0                0   \n",
       "..   ...              ...              ...              ...              ...   \n",
       "125  ...                1                0                0                0   \n",
       "126  ...                1                1                1                0   \n",
       "127  ...                1                1                1                1   \n",
       "128  ...                0                0                0                0   \n",
       "129  ...                0                1                0                0   \n",
       "\n",
       "     T2_KQ_13e_Score  T2_KQ_13f_Score  T2_KQ_13g_Score  T2_KQ_13h_Score  \\\n",
       "0                  0                0                0                0   \n",
       "1                  0                1                1                0   \n",
       "2                  0                0                0                0   \n",
       "3                  0                0                0                0   \n",
       "4                  0                0                0                0   \n",
       "..               ...              ...              ...              ...   \n",
       "125                0                0                0                0   \n",
       "126                0                0                1                1   \n",
       "127                1                1                1                1   \n",
       "128                0                0                0                0   \n",
       "129                0                1                0                1   \n",
       "\n",
       "     T2_KQ_13i_Score  T2_KQ_Total_Score  \n",
       "0                  0                  0  \n",
       "1                  1                 14  \n",
       "2                  0                  0  \n",
       "3                  0                  0  \n",
       "4                  0                  0  \n",
       "..               ...                ...  \n",
       "125                0                  5  \n",
       "126                1                 12  \n",
       "127                1                 16  \n",
       "128                0                  0  \n",
       "129                1                 11  \n",
       "\n",
       "[130 rows x 115 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu, chi2_contingency\n",
    "\n",
    "# Load the dataset\n",
    "data_path = '/Users/nagesh/Library/CloudStorage/OneDrive-NextGenMetricsB.V/Personal/Anisha/M3/data.xlsx'\n",
    "try:\n",
    "    # Attempt to read the Excel file\n",
    "    data = pd.read_excel(data_path)\n",
    "    # If successful, display the first few rows of the dataframe\n",
    "    message = \"Excel file read successfully! Here are the first few rows of the data:\"\n",
    "    preview = data.head()\n",
    "except Exception as e:\n",
    "    # If reading fails, display an error message\n",
    "    message = f\"An error occurred: {e}\"\n",
    "    preview = None\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 130 entries, 0 to 129\n",
      "Data columns (total 115 columns):\n",
      " #    Column              Dtype  \n",
      "---   ------              -----  \n",
      " 0    groupID             int64  \n",
      " 1    Patient_ID          int64  \n",
      " 2    IGVZ_intervention   int64  \n",
      " 3    Years_of_education  int64  \n",
      " 4    Country_of_origin   int64  \n",
      " 5    Gravida             int64  \n",
      " 6    Para                int64  \n",
      " 7    T1_Gestational_age  float64\n",
      " 8    T1_KQ_1             int64  \n",
      " 9    T1_KQ_2             int64  \n",
      " 10   T1_KQ_3             int64  \n",
      " 11   T1_KQ_4             int64  \n",
      " 12   T1_KQ_5             int64  \n",
      " 13   T1_KQ_6             int64  \n",
      " 14   T1_KQ_7             int64  \n",
      " 15   T1_KQ_8             int64  \n",
      " 16   T1_KQ_9             int64  \n",
      " 17   T1_KQ_10            int64  \n",
      " 18   T1_KQ_11            int64  \n",
      " 19   T1_KQ_12            int64  \n",
      " 20   T1_KQ_13a           int64  \n",
      " 21   T1_KQ_13b           int64  \n",
      " 22   T1_KQ_13c           int64  \n",
      " 23   T1_KQ_13d           int64  \n",
      " 24   T1_KQ_13e           int64  \n",
      " 25   T1_KQ_13f           int64  \n",
      " 26   T1_KQ_13g           int64  \n",
      " 27   T1_KQ_13h           int64  \n",
      " 28   T1_KQ_13i           int64  \n",
      " 29   T2_KQ_1             int64  \n",
      " 30   T2_KQ_2             int64  \n",
      " 31   T2_KQ_3             int64  \n",
      " 32   T2_KQ_4             int64  \n",
      " 33   T2_KQ_5             int64  \n",
      " 34   T2_KQ_6             int64  \n",
      " 35   T2_KQ_7             int64  \n",
      " 36   T2_KQ_8             int64  \n",
      " 37   T2_KQ_9             int64  \n",
      " 38   T2_KQ_10            int64  \n",
      " 39   T2_KQ_11            int64  \n",
      " 40   T2_KQ_12            int64  \n",
      " 41   T2_KQ_13a           int64  \n",
      " 42   T2_KQ_13b           int64  \n",
      " 43   T2_KQ_13c           int64  \n",
      " 44   T2_KQ_13d           int64  \n",
      " 45   T2_KQ_13e           int64  \n",
      " 46   T2_KQ_13f           int64  \n",
      " 47   T2_KQ_13g           int64  \n",
      " 48   T2_KQ_13h           int64  \n",
      " 49   T2_KQ_13i           int64  \n",
      " 50   T1_Lady_X1          int64  \n",
      " 51   T1_Lady_X2          int64  \n",
      " 52   T1_Lady_X3          int64  \n",
      " 53   T1_Lady_X4          int64  \n",
      " 54   T1_Lady_X5          int64  \n",
      " 55   T2_Lady_X1          int64  \n",
      " 56   T2_Lady_X2          int64  \n",
      " 57   T2_Lady_X3          int64  \n",
      " 58   T2_Lady_X4          int64  \n",
      " 59   T2_Lady_X5          int64  \n",
      " 60   T3_Breastfeeding    int64  \n",
      " 61   T3_Lady_X1          int64  \n",
      " 62   T3_Lady_X2          int64  \n",
      " 63   T3_Lady_X3          int64  \n",
      " 64   T3_Lady_X4          int64  \n",
      " 65   T3_Lady_X5          int64  \n",
      " 66   T3_Lady_X6          int64  \n",
      " 67   T3_Lady_X7          int64  \n",
      " 68   Knowledge_complete  int64  \n",
      " 69   T2_ladyx_complete   int64  \n",
      " 70   T3_complete         int64  \n",
      " 71   T1_KQ_1_Score       int64  \n",
      " 72   T1_KQ_2_Score       int64  \n",
      " 73   T1_KQ_3_Score       int64  \n",
      " 74   T1_KQ_4_Score       int64  \n",
      " 75   T1_KQ_5_Score       int64  \n",
      " 76   T1_KQ_6_Score       int64  \n",
      " 77   T1_KQ_7_Score       int64  \n",
      " 78   T1_KQ_8_Score       int64  \n",
      " 79   T1_KQ_9_Score       int64  \n",
      " 80   T1_KQ_10_Score      int64  \n",
      " 81   T1_KQ_11_Score      int64  \n",
      " 82   T1_KQ_12_Score      int64  \n",
      " 83   T1_KQ_13a_Score     int64  \n",
      " 84   T1_KQ_13b_Score     int64  \n",
      " 85   T1_KQ_13c_Score     int64  \n",
      " 86   T1_KQ_13d_Score     int64  \n",
      " 87   T1_KQ_13e_Score     int64  \n",
      " 88   T1_KQ_13f_Score     int64  \n",
      " 89   T1_KQ_13g_Score     int64  \n",
      " 90   T1_KQ_13h_Score     int64  \n",
      " 91   T1_KQ_13i_Score     int64  \n",
      " 92   T1_KQ_Total_Score   int64  \n",
      " 93   T2_KQ_1_Score       int64  \n",
      " 94   T2_KQ_2_Score       int64  \n",
      " 95   T2_KQ_3_Score       int64  \n",
      " 96   T2_KQ_4_Score       int64  \n",
      " 97   T2_KQ_5_Score       int64  \n",
      " 98   T2_KQ_6_Score       int64  \n",
      " 99   T2_KQ_7_Score       int64  \n",
      " 100  T2_KQ_8_Score       int64  \n",
      " 101  T2_KQ_9_Score       int64  \n",
      " 102  T2_KQ_10_Score      int64  \n",
      " 103  T2_KQ_11_Score      int64  \n",
      " 104  T2_KQ_12_Score      int64  \n",
      " 105  T2_KQ_13a_Score     int64  \n",
      " 106  T2_KQ_13b_Score     int64  \n",
      " 107  T2_KQ_13c_Score     int64  \n",
      " 108  T2_KQ_13d_Score     int64  \n",
      " 109  T2_KQ_13e_Score     int64  \n",
      " 110  T2_KQ_13f_Score     int64  \n",
      " 111  T2_KQ_13g_Score     int64  \n",
      " 112  T2_KQ_13h_Score     int64  \n",
      " 113  T2_KQ_13i_Score     int64  \n",
      " 114  T2_KQ_Total_Score   int64  \n",
      "dtypes: float64(1), int64(114)\n",
      "memory usage: 116.9 KB\n"
     ]
    }
   ],
   "source": [
    "# Display info with verbose=False\n",
    "data.info(verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the number of Treatment Group v/s Control Group participants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the number of members in the treatment group (groupID == 1)\n",
    "treatment_group_count = data[data['groupID'] == 1].shape[0]\n",
    "print('treatment group participants:', treatment_group_count)\n",
    "\n",
    "control_group_count = data[data['groupID'] == 0].shape[0]\n",
    "print('control group participants:', control_group_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu, chi2_contingency\n",
    "\n",
    "# Load your data\n",
    "#data = pd.read_csv('path_to_your_csv_file.csv')\n",
    "\n",
    "# Define the parameters to compare\n",
    "parameters = ['Years_of_education', 'Country_of_origin', 'Gravida', 'Para', 'T1_Gestational_age']\n",
    "\n",
    "# Assuming 'groupID' column with 1 for treatment and 0 for control group\n",
    "treatment_group = data[data['groupID'] == 1]\n",
    "control_group = data[data['groupID'] == 0]\n",
    "\n",
    "# Initialize a dictionary to store p-values\n",
    "p_values = {}\n",
    "\n",
    "# Iterate over the parameters and perform the appropriate statistical test\n",
    "for param in parameters:\n",
    "    if param == 'Country_of_origin':  # Assuming 'Country_of_origin' should be treated as categorical\n",
    "        # Create a contingency table\n",
    "        contingency_table = pd.crosstab(data['groupID'], data[param])\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "        p_values[param] = p\n",
    "    else:\n",
    "        # Assume other variables are continuous and use Mann-Whitney U test\n",
    "        stat, p = mannwhitneyu(treatment_group[param], control_group[param], alternative='two-sided')\n",
    "        p_values[param] = p\n",
    "\n",
    "# Output the p-values\n",
    "print(p_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapiro-Wilk test for detecting Normality in distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Excel sheet has been read successfully. It contains data including group IDs, patient IDs, information on whether the IGVZ intervention was received, years of education, country of origin, gravida, para, gestational age at two time points (T1 and T2), and scores on a knowledge questionnaire at both time points, among other variables.\n",
    "\n",
    "We can now proceed with the Shapiro-Wilk test for normality on the appropriate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "# Identify columns related to T1 scores\n",
    "t1_score_columns = [col for col in data.columns if 'T1_' in col and '_Score' in col]\n",
    "\n",
    "# Initialize a dictionary to hold the p-values of the Shapiro-Wilk test for normality\n",
    "shapiro_p_values = {}\n",
    "\n",
    "# Perform Shapiro-Wilk test for each T1 score column and store the p-values\n",
    "for col in t1_score_columns:\n",
    "    shapiro_test = shapiro(data[col])\n",
    "    shapiro_p_values[col] = \"{:.17f}\".format(shapiro_test.pvalue)\n",
    "\n",
    "shapiro_p_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T2 Scores (Post Intervention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "# Identify columns related to T1 scores\n",
    "t1_score_columns = [col for col in data.columns if 'T2_' in col and '_Score' in col]\n",
    "\n",
    "# Initialize a dictionary to hold the p-values of the Shapiro-Wilk test for normality\n",
    "shapiro_p_values = {}\n",
    "\n",
    "# Perform Shapiro-Wilk test for each T1 score column and store the p-values\n",
    "for col in t1_score_columns:\n",
    "    shapiro_test = shapiro(data[col])\n",
    "    shapiro_p_values[col] = \"{:.17f}\".format(shapiro_test.pvalue)\n",
    "\n",
    "shapiro_p_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the Shapiro-Wilk test on the T2 scores also yields non-significant results (i.e., p-values less than your chosen significance level, often 0.05), it means that the data for the T2 knowledge scores also deviates from a normal distribution.\n",
    "\n",
    "In simpler terms, imagine you're sorting candies into different colored jars. For normally distributed data, the candies would be fairly evenly spread out across the jars. But if the data is not normally distributed, it's like most of the candies end up in one or two jars, and very few candies are in the other jars. This uneven distribution is what the Shapiro-Wilk test detects.\n",
    "\n",
    "When the data is not normally distributed, it suggests that the scores at T2 for both treatment and control groups don't follow the typical bell-shaped curve you might expect. Instead, the scores might be clustered at certain levels, with fewer participants scoring at the extremes.\n",
    "\n",
    "For analysis, this means you might want to use non-parametric tests, like the Mann-Whitney U test, to compare the T2 knowledge scores between the treatment and control groups. These tests don't rely on the assumption of normality and are robust to non-normal distributions. They assess differences in the rank ordering of scores rather than their distributional shape.\n",
    "\n",
    "In essence, if the T2 scores are also not normally distributed, you can still analyze the data using appropriate statistical methods. The key is to choose tests that don't require the assumption of normality and are suitable for the type of data you have.\n",
    "\n",
    "The Shapiro-Wilk test results for the T2 scores indicate extremely small p-values for each of the variables, with values ranging from 0.00000000000000000 to 0.00000008817142739. This indicates that the null hypothesis of normality is rejected for all T2 score columns.\n",
    "\n",
    "In simpler terms, it means that the data for the T2 knowledge scores does not follow a normal distribution. This suggests that the scores at T2 for both treatment and control groups are not spread out evenly but are instead clustered at certain levels, with fewer participants scoring at the extremes.\n",
    "\n",
    "Given that the T2 scores are not normally distributed, you would want to use non-parametric tests, such as the Mann-Whitney U test, to compare the T2 knowledge scores between the treatment and control groups. These tests do not rely on the assumption of normality and are suitable for analyzing non-normally distributed data.\n",
    "\n",
    "When interpreting the results of the Mann-Whitney U test, focus on comparing the median scores between the treatment and control groups rather than the means, as the test assesses differences in the rank ordering of scores rather than their distributional shape.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapiro-Wilk Test for residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the change in total knowledge scores from T1 to T2 for each participant\n",
    "data['Change_in_KQ_Total_Score'] = data['T2_KQ_Total_Score'] - data['T1_KQ_Total_Score']\n",
    "\n",
    "# Step 2: Calculate the mean change for both the intervention group and control group\n",
    "mean_change_intervention = data[data['groupID'] == 1]['Change_in_KQ_Total_Score'].mean()\n",
    "mean_change_control = data[data['groupID'] == 0]['Change_in_KQ_Total_Score'].mean()\n",
    "\n",
    "# Step 3: Compute residuals\n",
    "data['Residuals'] = data.apply(lambda row: row['Change_in_KQ_Total_Score'] - (mean_change_intervention if row['groupID'] == 1 else mean_change_control), axis=1)\n",
    "\n",
    "# Step 4: Perform the Shapiro-Wilk test on these residuals\n",
    "shapiro_test_resid = shapiro(data['Residuals'])\n",
    "shapiro_test_resid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shapiro-Wilk test result shows two key pieces of information:\n",
    "\n",
    "Statistic = 0.4353: This is the test statistic value, which measures how closely your data's distribution matches a normal distribution. Values closer to 1 suggest a distribution closer to normal.\n",
    "P-value = 3.07e-20: This is an extremely small number, practically zero, which indicates the probability of observing your data if it were drawn from a normal distribution.\n",
    "Given that the p-value is much less than the typical alpha level of 0.05, you can reject the null hypothesis of the Shapiro-Wilk test, which states that the data is normally distributed. This means that the residuals from the changes in total knowledge scores between T1 and T2 for your groups do not follow a normal distribution.\n",
    "\n",
    "How to Proceed Further\n",
    "\n",
    "Given the non-normal distribution of your residuals, proceeding with parametric tests that assume normality (like the two-way repeated measures ANOVA) might not be the most appropriate choice. However, if two-way repeated measures ANOVA is \"robust\" to violations of normality, meaning it can still provide valid results when the data is not perfectly normal but close (which is clearly not the case)\n",
    "\n",
    "the situation, indicated by the Shapiro-Wilk test result, shows a significant deviation from normality. Here are some suggestions on how to proceed:\n",
    "\n",
    "Transformation of Data: Consider transforming your data to achieve normality. Common transformations include log, square root, or inverse. After transformation, you can perform the Shapiro-Wilk test again to check for normality.\n",
    "\n",
    "Non-Parametric Alternatives: Since your data significantly deviates from normality, look for non-parametric alternatives. For repeated measures data, the Friedman test is a non-parametric alternative for comparing more than two related groups. However, for analyzing the interaction effect (group x time) without assuming normality, options are more limited.\n",
    "\n",
    "Robust Methods: Explore robust statistical methods that are less sensitive to violations of normality assumptions. These methods provide ways to analyze data that do not meet the assumptions required for traditional parametric tests.\n",
    "\n",
    "Consult with Your Supervisor: Share these findings and discuss the significant deviation from normality. Your supervisor may agree to proceed with the analysis acknowledging the data's limitations or may suggest an alternative approach.\n",
    "\n",
    "Report Findings: Regardless of the chosen method, clearly report the findings from the Shapiro-Wilk test in your methods section. Explain how the results influenced your choice of statistical tests and discuss the implications for interpreting your study's findings.\n",
    "\n",
    "Considering the advice to use a two-way repeated measures ANOVA, you might still discuss its use with your supervisor, especially focusing on its robustness and any potential impact the violation of normality could have on your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the total scores using Square Root transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Apply a square root transformation to the total knowledge scores at T1 and T2\n",
    "sqrt_transformed_T1_scores = np.sqrt(data['T1_KQ_Total_Score'])\n",
    "sqrt_transformed_T2_scores = np.sqrt(data['T2_KQ_Total_Score'])\n",
    "\n",
    "# Perform the Shapiro-Wilk test on the transformed data for both T1 and T2\n",
    "shapiro_test_T1_sqrt_transformed = shapiro(sqrt_transformed_T1_scores)\n",
    "shapiro_test_T2_sqrt_transformed = shapiro(sqrt_transformed_T2_scores)\n",
    "\n",
    "(shapiro_test_T1_sqrt_transformed, shapiro_test_T2_sqrt_transformed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the Shapiro-Wilk test on your square root-transformed total knowledge scores for T1 and T2 indicate that both transformed distributions still deviate significantly from normality:\n",
    "\n",
    "T1 Transformed Scores:\n",
    "\n",
    "Statistic: 0.6782\n",
    "P-value: 1.68e-15\n",
    "T2 Transformed Scores:\n",
    "\n",
    "Statistic: 0.7451\n",
    "P-value: 9.77e-14\n",
    "In both cases, the test statistics (0.6782 for T1 and 0.7451 for T2) are far from 1, indicating a departure from normality. Furthermore, the extremely small p-values (far below the conventional alpha level of 0.05) lead us to reject the null hypothesis that the data is normally distributed.\n",
    "\n",
    "Implications and Next Steps\n",
    "\n",
    "Given these results, it's clear that even after a square root transformation, the distributions of your total knowledge scores at T1 and T2 are not normal. This suggests a need for either further transformation attempts or the consideration of non-parametric statistical tests that do not assume normality.\n",
    "\n",
    "Further Transformation Attempts: You could try other transformations, such as the logarithmic (if applicable), Box-Cox (if all values are positive), or even more robust methods like rank transformations. Each has its own conditions and suitability depending on the nature of the data's skewness and scale.\n",
    "\n",
    "Non-Parametric Tests: Given the challenges in achieving a normal distribution through transformations, non-parametric alternatives to your intended analyses might be more appropriate. These tests do not assume normality. For comparing two related samples (e.g., T1 vs. T2 scores within the same subjects), the Wilcoxon signed-rank test is a common choice. For comparing two independent samples (e.g., intervention vs. control groups), the Mann-Whitney U test is widely used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the total scores using Logarithmic transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for non-positive values in T1 and T2 total knowledge scores\n",
    "non_positive_T1 = any(data['T1_KQ_Total_Score'] <= 0)\n",
    "non_positive_T2 = any(data['T2_KQ_Total_Score'] <= 0)\n",
    "\n",
    "# Choose a transformation based on the presence of non-positive values\n",
    "if non_positive_T1 or non_positive_T2:\n",
    "    # If there are non-positive values, we will use a square root transformation after adding a constant to ensure positivity\n",
    "    constant = 1 - min(data['T1_KQ_Total_Score'].min(), data['T2_KQ_Total_Score'].min())\n",
    "    transformed_T1_scores = np.sqrt(data['T1_KQ_Total_Score'] + constant)\n",
    "    transformed_T2_scores = np.sqrt(data['T2_KQ_Total_Score'] + constant)\n",
    "else:\n",
    "    # If all values are positive, we can directly apply a logarithmic transformation\n",
    "    transformed_T1_scores = np.log(data['T1_KQ_Total_Score'])\n",
    "    transformed_T2_scores = np.log(data['T2_KQ_Total_Score'])\n",
    "\n",
    "# Perform the Shapiro-Wilk test on the transformed data for both T1 and T2\n",
    "shapiro_test_T1_transformed = shapiro(transformed_T1_scores)\n",
    "shapiro_test_T2_transformed = shapiro(transformed_T2_scores)\n",
    "\n",
    "(shapiro_test_T1_transformed, shapiro_test_T2_transformed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shapiro-Wilk test results for your logarithmically transformed total knowledge scores at T1 and T2 still indicate significant deviations from a normal distribution:\n",
    "\n",
    "T1 Log-Transformed Scores:\n",
    "\n",
    "Statistic: 0.6499\n",
    "P-value: 3.62e-16\n",
    "T2 Log-Transformed Scores:\n",
    "\n",
    "Statistic: 0.8020\n",
    "P-value: 5.83e-12\n",
    "Even after the logarithmic transformation, the test statistics (0.6499 for T1 and 0.8020 for T2) are below the threshold for normality, and the p-values are well below the commonly used alpha level of 0.05. This leads to the rejection of the null hypothesis that the data comes from a normally distributed population.\n",
    "\n",
    "Interpretation and Recommendations:\n",
    "Given that both square root and logarithmic transformations have not resulted in distributions that satisfy the criteria for normality, it suggests that your data inherently has properties that resist these transformations to normality. This could be due to extreme values, skewness, or the nature of the data itself.\n",
    "\n",
    "Non-Parametric Tests: At this point, considering non-parametric alternatives for your analysis would be advisable. These tests do not assume normal distribution and can provide robust insights even when parametric assumptions are not met. For comparisons within the same group over time, consider the Wilcoxon signed-rank test. For comparing independent groups, the Mann-Whitney U test could be suitable.\n",
    "\n",
    "Further Data Exploration: Beyond transformation and testing for normality, exploring your data for outliers, data entry errors, or subgroups that may behave differently could offer insights. Understanding the nature of your data more deeply can inform both the choice of statistical tests and the interpretation of your results.\n",
    "\n",
    "Data Transformation and Analysis Reporting: In your reporting, it's essential to detail the steps taken to assess normality and the rationale behind the chosen statistical methods. Transparency about the challenges encountered and how they were addressed adds to the credibility and reproducibility of your research.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets say we choose Parametric tests - JUST HYPOTHETICALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "\n",
    "# Prepare the data for two-way repeated measures ANOVA\n",
    "anova_data = data[['Patient_ID', 'groupID', 'T1_KQ_Total_Score', 'T2_KQ_Total_Score']].melt(id_vars=['Patient_ID', 'groupID'],\n",
    "                                                                                              var_name='Time',\n",
    "                                                                                              value_name='Total_Score')\n",
    "anova_data['Time'] = anova_data['Time'].map({'T1_KQ_Total_Score': 'T1', 'T2_KQ_Total_Score': 'T2'})\n",
    "\n",
    "# Encode 'groupID' as a categorical variable for clarity\n",
    "anova_data['groupID'] = anova_data['groupID'].astype('category')\n",
    "\n",
    "# Assuming 'anova_data' is your DataFrame prepared for ANOVA\n",
    "# Aggregate the data to ensure one observation per subject per condition\n",
    "#aggregated_data = anova_data.groupby(['Patient_ID', 'Time', 'groupID']).mean().fillna(0).reset_index()\n",
    "aggregated_data = anova_data.groupby(['Patient_ID', 'Time', 'groupID'], observed=False).mean().fillna(0).reset_index()\n",
    "\n",
    "# Perform two-way repeated measures ANOVA\n",
    "try:\n",
    "# Then, attempt the two-way repeated measures ANOVA again with the aggregated data\n",
    "    aovrm = AnovaRM(data=aggregated_data, depvar='Total_Score', subject='Patient_ID', within=['Time', 'groupID'])\n",
    "    res = aovrm.fit()\n",
    "    result = res.summary()\n",
    "except Exception as e:\n",
    "    result = f\"An error occurred: {e}\"\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOVA Results Interpretation\n",
    "\n",
    "Time (Effect of time on knowledge scores)\n",
    "\n",
    "F Value: 0.0172\n",
    "Degrees of Freedom (Num DF): 1\n",
    "Degrees of Freedom (Den DF): 127\n",
    "Pr > F (p-value): 0.8957\n",
    "Interpretation: The effect of time (from T1 to T2) on knowledge scores is not statistically significant (p > 0.05), suggesting that knowledge scores did not significantly change over time across all participants.\n",
    "groupID (Effect of being in the intervention vs. control group)\n",
    "\n",
    "F Value: 32.5714\n",
    "Degrees of Freedom (Num DF): 1\n",
    "Degrees of Freedom (Den DF): 127\n",
    "Pr > F (p-value): < 0.0001\n",
    "Interpretation: The effect of group membership (intervention vs. control) is statistically significant, indicating a significant difference in knowledge scores between the intervention and control groups across both time points. The high F value suggests a strong effect.\n",
    "Time:groupID (Interaction effect between time and groupID)\n",
    "\n",
    "F Value: 0.6185\n",
    "Degrees of Freedom (Num DF): 1\n",
    "Degrees of Freedom (Den DF): 127\n",
    "Pr > F (p-value): 0.4331\n",
    "Interpretation: The interaction between time and groupID is not statistically significant, suggesting that the change in knowledge scores from T1 to T2 does not differ significantly between the intervention and control groups.\n",
    "\n",
    "How to Proceed\n",
    "\n",
    "Given these results:\n",
    "\n",
    "Highlight the Group Effect: The significant difference between groups underscores the impact of the intervention. This is a key finding, as it suggests that participants in one group (presumably the intervention group) had different outcomes compared to the control group.\n",
    "\n",
    "Consider the Lack of Time Effect: The lack of significant change over time suggests that, on average, knowledge scores did not increase from T1 to T2. This could be due to various factors, including the possibility that the intervention did not have the desired effect, or that the effect was not captured between these two time points.\n",
    "\n",
    "Interaction Effect: The absence of a significant interaction effect indicates that the effect of the intervention did not significantly vary from T1 to T2 compared to the control. This could imply that the intervention's impact was consistent across time points or that any changes were not captured by the measurements used.\n",
    "\n",
    "Further Analysis and Reporting: These results provide valuable insights into the effectiveness of the intervention and its consistency over time. When reporting these findings, it's important to discuss the context and implications of the significant group effect, the reasons behind the lack of significant time effect, and why the interaction might not have been significant. Additionally, considering other variables (as previously discussed) might provide further insights.\n",
    "\n",
    "Explore Additional Variables: Given the significant group effect, exploring how other variables (e.g., Gravida, Para, country_of_origin) might influence the scores could provide deeper insights and help tailor future interventions more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, Why do we go back and choose Non-Parametric tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-Normal Distribution: The Shapiro-Wilk tests indicated that the knowledge scores at both T1 and T2 are not normally distributed. Therefore, parametric tests such as the t-test, which assume normality, may not be appropriate. Instead, non-parametric tests that do not rely on the normality assumption are more suitable.\n",
    "\n",
    "Comparison of Independent Groups: The Mann-Whitney U test is a non-parametric test used to compare the distribution of a continuous variable between two independent groups. In this case, we want to compare the knowledge scores between the treatment and control groups at both T1 and T2. The Mann-Whitney U test allows us to assess whether there are significant differences in knowledge levels between the two groups.\n",
    "\n",
    "Comparison of Paired Groups: The Wilcoxon signed-rank test is a non-parametric test used to compare the distribution of a continuous variable between two paired groups. We can use this test to assess whether there are significant changes in knowledge scores within each group from T1 to T2. By comparing the scores within each group over time, we can evaluate the effectiveness of the intervention (IGVZ) in improving knowledge levels among pregnant asylum seekers.\n",
    "\n",
    "Robustness to Outliers: Both the Mann-Whitney U test and the Wilcoxon signed-rank test are robust to outliers, which is important when dealing with real-world data that may contain extreme values or skewness.\n",
    "\n",
    "Assumption of Random Sampling: Both tests do not assume that the data is drawn from a normally distributed population, making them suitable for the study's dataset, which may not adhere to strict assumptions of random sampling.\n",
    "\n",
    "By choosing these non-parametric tests, we ensure that our statistical analyses are appropriate for the nature of the data and provide reliable results even when the normality assumption is violated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mann Whitney U Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Separate the data into treatment and control groups\n",
    "treatment_group = data[data['groupID'] == 1]\n",
    "control_group = data[data['groupID'] == 0]\n",
    "\n",
    "# Define the columns for T1 and T2 scores\n",
    "t1_columns = [col for col in data.columns if 'T1_KQ_' in col and 'Score' in col]\n",
    "t2_columns = [col for col in data.columns if 'T2_KQ_' in col and 'Score' in col]\n",
    "\n",
    "# Function to print results\n",
    "def print_test_results(group, col, stat, p_value, treatment_median, control_median):\n",
    "    print(f\"Mann-Whitney U test for {col} ({group}):\")\n",
    "    print(f\"   U Statistic: {stat:.2f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    print(f\"   Treatment Group Median: {treatment_median}\")\n",
    "    print(f\"   Control Group Median: {control_median}\\n\")\n",
    "\n",
    "# Perform Mann-Whitney U test for T1 scores and compare medians\n",
    "for col in t1_columns:\n",
    "    stat, p_value = mannwhitneyu(treatment_group[col], control_group[col])\n",
    "    treatment_median = treatment_group[col].median()\n",
    "    control_median = control_group[col].median()\n",
    "    print_test_results(\"T1\", col, stat, p_value, treatment_median, control_median)\n",
    "\n",
    "# Perform Mann-Whitney U test for T2 scores and compare medians\n",
    "for col in t2_columns:\n",
    "    stat, p_value = mannwhitneyu(treatment_group[col], control_group[col])\n",
    "    treatment_median = treatment_group[col].median()\n",
    "    control_median = control_group[col].median()\n",
    "    print_test_results(\"T2\", col, stat, p_value, treatment_median, control_median)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Findings:\n",
    "\n",
    "Most Scores Show No Significant Difference: Many of the p-values are above the conventional threshold of 0.05, indicating no statistically significant difference in the rank ordering of scores between the treatment and control groups for those questions. This is reflected in the median scores being equal for most questions.\n",
    "\n",
    "Significant Differences in Specific Questions and Overall Scores: There are notable exceptions where the Mann-Whitney U test indicates a statistically significant difference between the groups. This includes:\n",
    "\n",
    "T1_KQ_7_Score: Lower median in the treatment group (0.0 vs. 1.0, p=0.0159), suggesting the control group had higher scores.\n",
    "T1_KQ_11_Score: No median difference (0.0 for both groups), but p=0.0063 indicates a difference in rank ordering.\n",
    "T1_KQ_13b_Score: Equal medians (1.0 for both), but p=0.0067 suggests rank differences.\n",
    "T1_KQ_13i_Score: Equal medians (1.0 for both), but p=0.0183 suggests rank differences.\n",
    "T2_KQ_2_Score: Higher median in the treatment group (1.0 vs. 0.0, p=0.0001), indicating the treatment group had higher scores.\n",
    "T2_KQ_3_Score, T2_KQ_4_Score, T2_KQ_7_Score, T2_KQ_10_Score, T2_KQ_12_Score, T2_KQ_13d_Score, T2_KQ_13g_Score, and T2_KQ_Total_Score show significant differences favoring the treatment group in several instances, particularly notable in the total scores for T2 (12.0 vs. 10.0, p=0.0133).\n",
    "\n",
    "# Interpretation:\n",
    "\n",
    "Treatment Effectiveness: The significant results, especially in T2 where the treatment group often shows a higher median score, suggest the intervention might have positively influenced certain knowledge areas. This is particularly evident in the total score for T2, indicating a broader impact of the treatment on overall knowledge by the second measurement point.\n",
    "\n",
    "Rank Order vs. Median Scores: While the Mann-Whitney U test focuses on rank differences, observing the median scores provides a straightforward comparison of central tendency between groups. In cases where the p-value is significant but medians are the same, it suggests that while the median might not differ, the overall ranking of scores does, indicating a difference in distribution beyond the central point.\n",
    "\n",
    "Reporting and Further Analysis: When reporting these findings, itâ€™s crucial to note both the statistical significance (or lack thereof) and the practical significance, as indicated by differences in median scores. Where significant differences are observed, discussing the potential implications for the intervention's impact on knowledge acquisition will be key. Further, exploring why certain areas showed improvement while others did not could provide insights into refining or focusing future interventions.\n",
    "\n",
    "This analysis highlights the nuanced impact of your intervention, demonstrating areas of significant knowledge gain and offering a basis for further investigation into the intervention's effects and potential improvements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wilcoxon Signed-Rank Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# Perform Wilcoxon signed-rank test for T1 vs. T2 scores within treatment group\n",
    "for t1_col, t2_col in zip(t1_columns, t2_columns):\n",
    "    stat_treatment, p_value_treatment = wilcoxon(data[data['groupID'] == 1][t1_col], data[data['groupID'] == 1][t2_col])\n",
    "    print(f\"Wilcoxon signed-rank test for {t1_col} vs. {t2_col} (Treatment Group):\")\n",
    "    print(f\"   Statistic: {stat_treatment}\")\n",
    "    print(f\"   p-value: {p_value_treatment}\")\n",
    "\n",
    "# Perform Wilcoxon signed-rank test for T1 vs. T2 scores within control group\n",
    "for t1_col, t2_col in zip(t1_columns, t2_columns):\n",
    "    stat_control, p_value_control = wilcoxon(data[data['groupID'] == 0][t1_col], data[data['groupID'] == 0][t2_col])\n",
    "    print(f\"Wilcoxon signed-rank test for {t1_col} vs. {t2_col} (Control Group):\")\n",
    "    print(f\"   Statistic: {stat_control}\")\n",
    "    print(f\"   p-value: {p_value_control}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu, chi2_contingency\n",
    "\n",
    "# Load the data\n",
    "#data = pd.read_csv('/mnt/data/data.csv')\n",
    "\n",
    "# Define the parameters to compare\n",
    "parameters = ['Years_of_education', 'Country_of_origin', 'Gravida', 'Para', 'T1_Gestational_age']\n",
    "\n",
    "# Initialize a DataFrame to store the comparison results\n",
    "comparison_results = pd.DataFrame(columns=['Parameter', 'Group', 'Median', 'IQR', 'P-value'])\n",
    "\n",
    "# Function to perform the Mann-Whitney U test and return median, IQR, and p-value\n",
    "def compare_groups(parameter):\n",
    "    group1 = data[data['groupID'] == 1][parameter]\n",
    "    group0 = data[data['groupID'] == 0][parameter]\n",
    "    \n",
    "    # Calculate median and interquartile range (IQR)\n",
    "    median1, iqr1 = group1.median(), group1.quantile(0.75) - group1.quantile(0.25)\n",
    "    median0, iqr0 = group0.median(), group0.quantile(0.75) - group0.quantile(0.25)\n",
    "    \n",
    "    # Perform the Mann-Whitney U test\n",
    "    u_stat, p_val = mannwhitneyu(group1, group0)\n",
    "    \n",
    "    return median1, iqr1, median0, iqr0, p_val\n",
    "\n",
    "# Perform comparisons for each parameter\n",
    "for param in parameters:\n",
    "    median1, iqr1, median0, iqr0, p_val = compare_groups(param)\n",
    "    \n",
    "    # Append the results for treatment group\n",
    "    comparison_results = comparison_results.append({\n",
    "        'Parameter': param,\n",
    "        'Group': 'Treatment',\n",
    "        'Median': median1,\n",
    "        'IQR': iqr1,\n",
    "        'P-value': p_val\n",
    "    }, ignore_index=True)\n",
    "    \n",
    "    # Append the results for control group\n",
    "    comparison_results = comparison_results.append({\n",
    "        'Parameter': param,\n",
    "        'Group': 'Control',\n",
    "        'Median': median0,\n",
    "        'IQR': iqr0,\n",
    "        'P-value': p_val\n",
    "    }, ignore_index=True)\n",
    "\n",
    "comparison_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu, chi2_contingency\n",
    "\n",
    "# Load the data\n",
    "#data = pd.read_csv('your_data.csv')  # Make sure to use your actual file path here\n",
    "\n",
    "# Define a function to calculate medians, IQRs, and perform Mann-Whitney U test for continuous variables\n",
    "def analyze_continuous(data, group_col, continuous_col):\n",
    "    group_data = data.groupby(group_col)[continuous_col]\n",
    "    medians = group_data.median()\n",
    "    iqr = group_data.quantile(0.75) - group_data.quantile(0.25)\n",
    "    u_stat, p_value = mannwhitneyu(data[data[group_col] == 1][continuous_col],\n",
    "                                   data[data[group_col] == 0][continuous_col])\n",
    "    return medians, iqr, p_value\n",
    "\n",
    "# Define a function to create crosstabs and perform Chi-squared test for categorical variables\n",
    "def analyze_categorical(data, group_col, category_col):\n",
    "    crosstab = pd.crosstab(data[group_col], data[category_col])\n",
    "    chi2, p_value, _, _ = chi2_contingency(crosstab)\n",
    "    return crosstab, p_value\n",
    "\n",
    "# Define your groups and parameters\n",
    "groups = {1: 'Treatment', 0: 'Control'}\n",
    "parameters = {\n",
    "    'Years_of_education': 'continuous',\n",
    "    'Country_of_origin': 'categorical',\n",
    "    'Gravida': 'continuous',\n",
    "    'Para': 'continuous',\n",
    "    'T1_Gestational_age': 'continuous'\n",
    "}\n",
    "\n",
    "# Store the results\n",
    "results = {}\n",
    "\n",
    "# Analyze each parameter\n",
    "for parameter, p_type in parameters.items():\n",
    "    if p_type == 'continuous':\n",
    "        medians, iqr, p_value = analyze_continuous(data, 'groupID', parameter)\n",
    "        results[parameter] = {\n",
    "            'Medians': medians,\n",
    "            'IQRs': iqr,\n",
    "            'P-value': p_value\n",
    "        }\n",
    "    elif p_type == 'categorical':\n",
    "        crosstab, p_value = analyze_categorical(data, 'groupID', parameter)\n",
    "        results[parameter] = {\n",
    "            'Crosstab': crosstab,\n",
    "            'P-value': p_value\n",
    "        }\n",
    "\n",
    "# Now, `results` contains all the analysis information\n",
    "# You can print it or convert it into a DataFrame for a report\n",
    "# For instance, to print the median and IQR for 'Years_of_education':\n",
    "print('Years_of_education Medians:', results['Years_of_education']['Medians'])\n",
    "print('Years_of_education IQRs:', results['Years_of_education']['IQRs'])\n",
    "# To print the crosstab for 'Country_of_origin':\n",
    "print('Country_of_origin Crosstab:\\n', results['Country_of_origin']['Crosstab'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Load the data\n",
    "#data = pd.read_csv('/mnt/data/data.csv')\n",
    "\n",
    "# Define the parameters to compare\n",
    "parameters = ['Years_of_education', 'Country_of_origin', 'Gravida', 'Para', 'T1_Gestational_age']\n",
    "\n",
    "# Initialize an empty list to store the comparison results\n",
    "comparison_results = []\n",
    "\n",
    "# Function to perform the Mann-Whitney U test and return median, IQR, and p-value\n",
    "def compare_groups(parameter):\n",
    "    group1 = data[data['groupID'] == 1][parameter]\n",
    "    group0 = data[data['groupID'] == 0][parameter]\n",
    "    \n",
    "    # Calculate median and interquartile range (IQR)\n",
    "    median1, iqr1 = group1.median(), group1.quantile(0.75) - group1.quantile(0.25)\n",
    "    median0, iqr0 = group0.median(), group0.quantile(0.75) - group0.quantile(0.25)\n",
    "    \n",
    "    # Perform the Mann-Whitney U test\n",
    "    u_stat, p_val = mannwhitneyu(group1, group0)\n",
    "    \n",
    "    return median1, iqr1, median0, iqr0, p_val\n",
    "\n",
    "# Perform comparisons for each parameter\n",
    "for param in parameters:\n",
    "    median1, iqr1, median0, iqr0, p_val = compare_groups(param)\n",
    "    \n",
    "    # Add the results for treatment group\n",
    "    comparison_results.append({\n",
    "        'Parameter': param,\n",
    "        'Group': 'Treatment',\n",
    "        'Median': median1,\n",
    "        'IQR': iqr1,\n",
    "        'P-value': p_val\n",
    "    })\n",
    "    \n",
    "    # Add the results for control group\n",
    "    comparison_results.append({\n",
    "        'Parameter': param,\n",
    "        'Group': 'Control',\n",
    "        'Median': median0,\n",
    "        'IQR': iqr0,\n",
    "        'P-value': p_val\n",
    "    })\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "comparison_results_df = pd.DataFrame(comparison_results)\n",
    "\n",
    "comparison_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education = data.groupby(['groupID', 'Years_of_education']).agg('count').reset_index()\n",
    "education\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your parameters: Specify which are continuous and which are categorical\n",
    "continuous_parameters = ['Years_of_education', 'T1_Gestational_age']\n",
    "categorical_parameters = ['Country_of_origin', 'Gravida', 'Para']\n",
    "\n",
    "# Assuming 'groupID' column with 1 for treatment and 0 for control group\n",
    "treatment_group = data[data['groupID'] == 1]\n",
    "control_group = data[data['groupID'] == 0]\n",
    "\n",
    "# Initialize a DataFrame to store results\n",
    "results = []\n",
    "\n",
    "# Function to perform Mann-Whitney U Test\n",
    "def mann_whitney_u_test(group1, group2):\n",
    "    stat, p = mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "    return p\n",
    "\n",
    "# Analyze continuous parameters\n",
    "for param in continuous_parameters:\n",
    "    # Auto-binning continuous variables into quartiles\n",
    "    data[param + '_bin'] = pd.qcut(data[param], q=4, duplicates='drop')\n",
    "    \n",
    "    # Iterate over each bin and calculate p-values\n",
    "    for bin in data[param + '_bin'].unique():\n",
    "        group1 = treatment_group[treatment_group[param + '_bin'] == bin][param]\n",
    "        group2 = control_group[control_group[param + '_bin'] == bin][param]\n",
    "        p_value = mann_whitney_u_test(group1, group2)\n",
    "        results.append({'Parameter': param, 'Category/Bin': bin, 'P-value': p_value})\n",
    "    \n",
    "# Convert results to DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu, chi2_contingency\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the data has been loaded into a DataFrame named `data`\n",
    "# And 'groupID' indicates treatment (1) and control (0) groups\n",
    "\n",
    "# Define continuous and categorical parameters\n",
    "continuous_parameters = ['Years_of_education', 'T1_Gestational_age']\n",
    "categorical_parameters = ['Country_of_origin', 'Gravida', 'Para']\n",
    "\n",
    "# Prepare storage for the results\n",
    "results = []\n",
    "\n",
    "# Process continuous variables\n",
    "for param in continuous_parameters:\n",
    "    # Extract data for each group\n",
    "    treatment_data = treatment_group[param].dropna()\n",
    "    control_data = control_group[param].dropna()\n",
    "\n",
    "    # Mann-Whitney U test to compare distributions\n",
    "    if not treatment_data.empty and not control_data.empty:\n",
    "        p_value = mannwhitneyu(treatment_data, control_data).pvalue\n",
    "    else:\n",
    "        p_value = np.nan  # Not applicable if one of the groups has no data\n",
    "\n",
    "    results.append({\n",
    "        'Parameter': param,\n",
    "        'Category/Bin': 'Overall',\n",
    "        'P-value': p_value,\n",
    "        'Treatment Median': treatment_data.median(),\n",
    "        'Control Median': control_data.median()\n",
    "    })\n",
    "\n",
    "# Process categorical variables\n",
    "for param in categorical_parameters:\n",
    "    # Get unique categories across both groups\n",
    "    categories = np.union1d(treatment_group[param].unique(), control_group[param].unique())\n",
    "\n",
    "    for category in categories:\n",
    "        # Count occurrences in each group\n",
    "        treatment_count = (treatment_group[param] == category).sum()\n",
    "        control_count = (control_group[param] == category).sum()\n",
    "\n",
    "        # Chi-squared test expects a contingency table as input\n",
    "        contingency_table = np.array([[treatment_count, len(treatment_group) - treatment_count],\n",
    "                                      [control_count, len(control_group) - control_count]])\n",
    "\n",
    "        # Perform Chi-squared test if both groups have the category\n",
    "        if treatment_count > 0 and control_count > 0:\n",
    "            p_value = chi2_contingency(contingency_table, correction=False)[1]\n",
    "        else:\n",
    "            p_value = np.nan  # Not applicable if the category does not appear in one of the groups\n",
    "\n",
    "        results.append({\n",
    "            'Parameter': param,\n",
    "            'Category/Bin': category,\n",
    "            'P-value': \"{:.17f}\".format(p_value),\n",
    "            'Treatment Count': treatment_count,\n",
    "            'Control Count': control_count\n",
    "        })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = results_df[results_df['Parameter'] == 'Para'].drop(columns=['Treatment Median', 'Control Median']).reset_index()\n",
    "\n",
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_demographics(p, c):\n",
    "    \n",
    "    # Recategorize 'Para' and split data\n",
    "    if p == 'Para':\n",
    "        data['parameter'] = data[p].apply(categorize_para)\n",
    "    elif p == 'Gravida':\n",
    "        data['parameter'] = data[p].apply(categorize_para)\n",
    "    elif p == 'T1_Gestational_age':\n",
    "        data['parameter'] = data[p].apply(categorize_para)\n",
    "    elif p == 'Years_of_education':\n",
    "        data['parameter'] = data[p].apply(categorize_para)\n",
    "    \n",
    "    treatment_group = data[data['groupID'] == 1]\n",
    "    control_group = data[data['groupID'] == 0]\n",
    "\n",
    "    # Calculate frequencies for each category within both groups\n",
    "    treatment_freq = treatment_group['parameter'].value_counts().reset_index(name='case_group')\n",
    "    control_freq = control_group['parameter'].value_counts().reset_index(name='control_group')\n",
    "\n",
    "    # Correctly rename 'index' column to merge on 'Para_category'\n",
    "    treatment_freq.rename(columns={'index': 'parameter'}, inplace=True)\n",
    "    control_freq.rename(columns={'index': 'parameter'}, inplace=True)\n",
    "\n",
    "    # Merge treatment and control frequencies\n",
    "    merged_freq = pd.merge(treatment_freq, control_freq, on='parameter', how='outer').fillna(0)\n",
    "\n",
    "    # Calculate the Chi-square test for the merged frequencies\n",
    "    chi2, p, dof, expected = chi2_contingency(merged_freq[['case_group', 'control_group']])\n",
    "\n",
    "    # Add p-value to the merged frequencies DataFrame\n",
    "    merged_freq['p-value'] = \"{:.17f}\".format(p)\n",
    "\n",
    "    merged_freq['category'] = c\n",
    "\n",
    "    new_order = ['category', 'parameter', 'case_group', 'control_group', 'p-value']\n",
    "\n",
    "    # Rearrange columns using direct indexing\n",
    "    merged_freq = merged_freq[new_order]\n",
    "\n",
    "    # Now, df has columns ordered as specified in `new_order`\n",
    "    # Display the merged frequencies along with the p-value\n",
    "    print(\"Chi-square test p-value for differences across 'parameter':\", p)\n",
    "    return merged_freq.reset_index()\n",
    "\n",
    "# Define the categories for \"Para\"\n",
    "def categorize_para(value):\n",
    "    if value == 0:\n",
    "        return 'Nulliparous'\n",
    "    elif 1 <= value <= 3:\n",
    "        return 'Low Multiparous'\n",
    "    elif value > 3:\n",
    "        return 'Grand Multipara'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "    \n",
    "# Recategorize 'Gravida' and split data\n",
    "def categorize_gravida(gravida):\n",
    "    if gravida == 0:\n",
    "        return 'Nulligravida'\n",
    "    elif gravida == 1:\n",
    "        return 'Primigravida'\n",
    "    elif gravida == 2:\n",
    "        return 'Secundigravida'\n",
    "    elif 3 <= gravida <= 5:\n",
    "        return 'Multigravida'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "def categorize_gestational_age(gestational_age):\n",
    "    if gestational_age < 28:\n",
    "        return 'Extremely Preterm'\n",
    "    elif 28 <= gestational_age < 32:\n",
    "        return 'Very Preterm'\n",
    "    elif 32 <= gestational_age < 37:\n",
    "        return 'Moderate to Late Preterm'\n",
    "    elif 37 <= gestational_age < 39:\n",
    "        return 'Early Term'\n",
    "    elif 39 <= gestational_age < 41:\n",
    "        return 'Full Term'\n",
    "    elif 41 <= gestational_age < 42:\n",
    "        return 'Late Term'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "def categorize_years_of_education(years):\n",
    "    if years == 0:\n",
    "        return 'No Formal Education'\n",
    "    elif 1 <= years <= 6:\n",
    "        return 'Primary Education'\n",
    "    elif 7 <= years <= 12:\n",
    "        return 'Secondary Education'\n",
    "    elif 13 <= years <= 14:\n",
    "        return 'Post-secondary Non-tertiary Education'\n",
    "    elif 15 <= years <= 16:\n",
    "        return 'Short-cycle Tertiary Education'\n",
    "    elif 17 <= years <= 18:\n",
    "        return 'Bachelorâ€™s or Equivalent Level'\n",
    "    elif 19 <= years <= 20:\n",
    "        return 'Masterâ€™s or Equivalent Level'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recategorize 'Para' and split data\n",
    "data['parameter'] = data['Para'].apply(categorize_para)\n",
    "\n",
    "treatment_group = data[data['groupID'] == 1]\n",
    "control_group = data[data['groupID'] == 0]\n",
    "\n",
    "# Calculate frequencies for each category within both groups\n",
    "treatment_freq = treatment_group['parameter'].value_counts().reset_index(name='case_group')\n",
    "control_freq = control_group['parameter'].value_counts().reset_index(name='control_group')\n",
    "\n",
    "# Correctly rename 'index' column to merge on 'Para_category'\n",
    "treatment_freq.rename(columns={'index': 'parameter'}, inplace=True)\n",
    "control_freq.rename(columns={'index': 'parameter'}, inplace=True)\n",
    "\n",
    "# Merge treatment and control frequencies\n",
    "merged_freq = pd.merge(treatment_freq, control_freq, on='parameter', how='outer').fillna(0)\n",
    "\n",
    "# Calculate the Chi-square test for the merged frequencies\n",
    "chi2, p, dof, expected = chi2_contingency(merged_freq[['case_group', 'control_group']])\n",
    "\n",
    "# Add p-value to the merged frequencies DataFrame\n",
    "merged_freq['p-value'] = \"{:.3f}\".format(p)\n",
    "\n",
    "merged_freq['category'] = 'Para'\n",
    "\n",
    "new_order = ['category', 'parameter', 'case_group', 'control_group', 'p-value']\n",
    "\n",
    "# Rearrange columns using direct indexing\n",
    "merged_freq = merged_freq[new_order]\n",
    "\n",
    "# Now, df has columns ordered as specified in `new_order`\n",
    "# Display the merged frequencies along with the p-value\n",
    "print(\"Chi-square test p-value for differences across 'parameter':\", p)\n",
    "merged_freq.to_csv('/Users/nagesh/Library/CloudStorage/OneDrive-NextGenMetricsB.V/Personal/Anisha/M3/results.csv', mode='a', header=True, index=False)\n",
    "merged_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recategorize 'Gravida' and split data\n",
    "def categorize_gravida(gravida):\n",
    "    if gravida == 0:\n",
    "        return 'Nulligravida'\n",
    "    elif gravida == 1:\n",
    "        return 'Primigravida'\n",
    "    elif gravida == 2:\n",
    "        return 'Secundigravida'\n",
    "    elif 3 <= gravida <= 5:\n",
    "        return 'Multigravida'\n",
    "    else:\n",
    "        return 'Grand multigravida'\n",
    "\n",
    "# Apply the categorization to the 'Gravida' column\n",
    "data['parameter'] = data['Gravida'].apply(categorize_gravida)\n",
    "\n",
    "treatment_group = data[data['groupID'] == 1]\n",
    "control_group = data[data['groupID'] == 0]\n",
    "\n",
    "# Calculate frequencies for each category within both groups\n",
    "treatment_freq = treatment_group['parameter'].value_counts().reset_index(name='case_group')\n",
    "control_freq = control_group['parameter'].value_counts().reset_index(name='control_group')\n",
    "\n",
    "# Correctly rename 'index' column to merge on 'Para_category'\n",
    "treatment_freq.rename(columns={'index': 'parameter'}, inplace=True)\n",
    "control_freq.rename(columns={'index': 'parameter'}, inplace=True)\n",
    "\n",
    "# Merge treatment and control frequencies\n",
    "merged_freq = pd.merge(treatment_freq, control_freq, on='parameter', how='outer').fillna(0)\n",
    "\n",
    "# Calculate the Chi-square test for the merged frequencies\n",
    "chi2, p, dof, expected = chi2_contingency(merged_freq[['case_group', 'control_group']])\n",
    "\n",
    "# Add p-value to the merged frequencies DataFrame\n",
    "merged_freq['p-value'] = \"{:.3f}\".format(p)\n",
    "\n",
    "merged_freq['category'] = 'Gravida'\n",
    "\n",
    "new_order = ['category', 'parameter', 'case_group', 'control_group', 'p-value']\n",
    "\n",
    "# Rearrange columns using direct indexing\n",
    "merged_freq = merged_freq[new_order]\n",
    "\n",
    "# Display the merged frequencies along with the p-value\n",
    "print(\"Chi-square test p-value for differences across 'Gravida_category':\", p)\n",
    "merged_freq.to_csv('/Users/nagesh/Library/CloudStorage/OneDrive-NextGenMetricsB.V/Personal/Anisha/M3/results.csv', mode='a', header=False, index=False)\n",
    "\n",
    "merged_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_gestational_age(gestational_age):\n",
    "    if gestational_age < 28:\n",
    "        return 'Extremely Preterm'\n",
    "    elif 28 <= gestational_age < 32:\n",
    "        return 'Very Preterm'\n",
    "    elif 32 <= gestational_age < 37:\n",
    "        return 'Moderate to Late Preterm'\n",
    "    elif 37 <= gestational_age < 39:\n",
    "        return 'Early Term'\n",
    "    elif 39 <= gestational_age < 41:\n",
    "        return 'Full Term'\n",
    "    elif 41 <= gestational_age < 42:\n",
    "        return 'Late Term'\n",
    "    else:\n",
    "        return 'Post Term'\n",
    "\n",
    "# Apply the categorization to the 'T1_Gestational_age' column\n",
    "data['parameter'] = data['T1_Gestational_age'].apply(categorize_gestational_age)\n",
    "\n",
    "treatment_group = data[data['groupID'] == 1]\n",
    "control_group = data[data['groupID'] == 0]\n",
    "\n",
    "# Calculate frequencies for each category within both groups\n",
    "treatment_freq = treatment_group['parameter'].value_counts().reset_index(name='case_group')\n",
    "control_freq = control_group['parameter'].value_counts().reset_index(name='control_group')\n",
    "\n",
    "# Correctly rename 'index' column to merge on 'Para_category'\n",
    "treatment_freq.rename(columns={'index': 'parameter'}, inplace=True)\n",
    "control_freq.rename(columns={'index': 'parameter'}, inplace=True)\n",
    "\n",
    "# Merge treatment and control frequencies\n",
    "merged_freq = pd.merge(treatment_freq, control_freq, on='parameter', how='outer').fillna(0)\n",
    "\n",
    "# Calculate the Chi-square test for the merged frequencies\n",
    "chi2, p, dof, expected = chi2_contingency(merged_freq[['case_group', 'control_group']])\n",
    "\n",
    "# Add p-value to the merged frequencies DataFrame\n",
    "merged_freq['p-value'] = \"{:.3f}\".format(p)\n",
    "\n",
    "merged_freq['category'] = 'Gestational Age'\n",
    "\n",
    "new_order = ['category', 'parameter', 'case_group', 'control_group', 'p-value']\n",
    "\n",
    "# Rearrange columns using direct indexing\n",
    "merged_freq = merged_freq[new_order]\n",
    "\n",
    "\n",
    "# Display the merged frequencies along with the p-value\n",
    "print(\"Chi-square test p-value for differences across 'T1_Gestational_age_category':\", p)\n",
    "merged_freq.to_csv('/Users/nagesh/Library/CloudStorage/OneDrive-NextGenMetricsB.V/Personal/Anisha/M3/results.csv', mode='a', header=False, index=False)\n",
    "merged_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_years_of_education(years):\n",
    "    if years == 0:\n",
    "        return 'No Formal Education'\n",
    "    elif 1 <= years <= 6:\n",
    "        return 'Primary Education'\n",
    "    elif 7 <= years <= 12:\n",
    "        return 'Secondary Education'\n",
    "    elif 13 <= years <= 14:\n",
    "        return 'Post-secondary Non-tertiary Education'\n",
    "    elif 15 <= years <= 16:\n",
    "        return 'Short-cycle Tertiary Education'\n",
    "    elif 17 <= years <= 18:\n",
    "        return 'Bachelorâ€™s or Equivalent Level'\n",
    "    elif 19 <= years <= 20:\n",
    "        return 'Masterâ€™s or Equivalent Level'\n",
    "    else:\n",
    "        return 'Doctoral or Equivalent Level'\n",
    "\n",
    "# Apply the categorization to the 'Years_of_education' column\n",
    "data['parameter'] = data['Years_of_education'].apply(categorize_years_of_education)\n",
    "\n",
    "treatment_group = data[data['groupID'] == 1]\n",
    "control_group = data[data['groupID'] == 0]\n",
    "\n",
    "# Calculate frequencies for each category within both groups\n",
    "treatment_freq = treatment_group['parameter'].value_counts().reset_index(name='case_group')\n",
    "control_freq = control_group['parameter'].value_counts().reset_index(name='control_group')\n",
    "\n",
    "# Correctly rename 'index' column to merge on 'Para_category'\n",
    "treatment_freq.rename(columns={'index': 'parameter'}, inplace=True)\n",
    "control_freq.rename(columns={'index': 'parameter'}, inplace=True)\n",
    "\n",
    "# Merge treatment and control frequencies\n",
    "merged_freq = pd.merge(treatment_freq, control_freq, on='parameter', how='outer').fillna(0)\n",
    "\n",
    "# Calculate the Chi-square test for the merged frequencies\n",
    "chi2, p, dof, expected = chi2_contingency(merged_freq[['case_group', 'control_group']])\n",
    "\n",
    "# Add p-value to the merged frequencies DataFrame\n",
    "merged_freq['p-value'] = \"{:.3f}\".format(p)\n",
    "\n",
    "merged_freq['category'] = 'Gestational Age'\n",
    "\n",
    "new_order = ['category', 'parameter', 'case_group', 'control_group', 'p-value']\n",
    "\n",
    "# Rearrange columns using direct indexing\n",
    "merged_freq = merged_freq[new_order]\n",
    "\n",
    "# Display the merged frequencies along with the p-value\n",
    "print(\"Chi-square test p-value for differences across 'Years_of_education_category':\", p)\n",
    "merged_freq.to_csv('/Users/nagesh/Library/CloudStorage/OneDrive-NextGenMetricsB.V/Personal/Anisha/M3/results.csv', mode='a', header=False, index=False)\n",
    "\n",
    "merged_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_group = data[data['groupID'] == 1]\n",
    "control_group = data[data['groupID'] == 0]\n",
    "\n",
    "# Calculate frequencies for each category within both groups\n",
    "treatment_freq = treatment_group['Country_of_origin'].value_counts().reset_index(name='case_group')\n",
    "control_freq = control_group['Country_of_origin'].value_counts().reset_index(name='control_group')\n",
    "\n",
    "# Correctly rename 'index' column to merge on 'Para_category'\n",
    "treatment_freq.rename(columns={'index': 'Country_of_origin'}, inplace=True)\n",
    "control_freq.rename(columns={'index': 'Country_of_origin'}, inplace=True)\n",
    "\n",
    "# Merge treatment and control frequencies\n",
    "merged_freq = pd.merge(treatment_freq, control_freq, on='Country_of_origin', how='outer').fillna(0)\n",
    "\n",
    "# Calculate the Chi-square test for the merged frequencies\n",
    "chi2, p, dof, expected = chi2_contingency(merged_freq[['case_group', 'control_group']])\n",
    "\n",
    "# Add p-value to the merged frequencies DataFrame\n",
    "merged_freq['p-value'] = \"{:.3f}\".format(p)\n",
    "\n",
    "merged_freq['category'] = 'Country'\n",
    "\n",
    "new_order = ['category', 'Country_of_origin', 'case_group', 'control_group', 'p-value']\n",
    "\n",
    "# Rearrange columns using direct indexing\n",
    "merged_freq = merged_freq[new_order]\n",
    "\n",
    "# Display the merged frequencies along with the p-value\n",
    "print(\"Chi-square test p-value for differences across 'Country_of_origin':\", p)\n",
    "merged_freq.to_csv('/Users/nagesh/Library/CloudStorage/OneDrive-NextGenMetricsB.V/Personal/Anisha/M3/results.csv', mode='a', header=False, index=False)\n",
    "\n",
    "merged_freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct Answer analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['groupID', 'T1_KQ_1_Score', 'T1_KQ_2_Score', 'T1_KQ_3_Score', 'T1_KQ_4_Score', 'T1_KQ_5_Score', 'T1_KQ_6_Score', 'T1_KQ_7_Score', 'T1_KQ_8_Score', 'T1_KQ_9_Score', 'T1_KQ_10_Score', 'T1_KQ_11_Score', 'T1_KQ_12_Score', 'T1_KQ_13a_Score', 'T1_KQ_13b_Score', 'T1_KQ_13c_Score', 'T1_KQ_13d_Score', 'T1_KQ_13e_Score', 'T1_KQ_13f_Score', 'T1_KQ_13g_Score', 'T1_KQ_13h_Score', 'T1_KQ_13i_Score', 'T1_KQ_Total_Score']\n",
      "['groupID', 'T2_KQ_1_Score', 'T2_KQ_2_Score', 'T2_KQ_3_Score', 'T2_KQ_4_Score', 'T2_KQ_5_Score', 'T2_KQ_6_Score', 'T2_KQ_7_Score', 'T2_KQ_8_Score', 'T2_KQ_9_Score', 'T2_KQ_10_Score', 'T2_KQ_11_Score', 'T2_KQ_12_Score', 'T2_KQ_13a_Score', 'T2_KQ_13b_Score', 'T2_KQ_13c_Score', 'T2_KQ_13d_Score', 'T2_KQ_13e_Score', 'T2_KQ_13f_Score', 'T2_KQ_13g_Score', 'T2_KQ_13h_Score', 'T2_KQ_13i_Score', 'T2_KQ_Total_Score']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 24 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   groupID         4 non-null      int64 \n",
      " 1   moment          4 non-null      object\n",
      " 2   Question 1      4 non-null      int64 \n",
      " 3   Question 2      4 non-null      int64 \n",
      " 4   Question 3      4 non-null      int64 \n",
      " 5   Question 4      4 non-null      int64 \n",
      " 6   Question 5      4 non-null      int64 \n",
      " 7   Question 6      4 non-null      int64 \n",
      " 8   Question 7      4 non-null      int64 \n",
      " 9   Question 8      4 non-null      int64 \n",
      " 10  Question 9      4 non-null      int64 \n",
      " 11  Question 10     4 non-null      int64 \n",
      " 12  Question 11     4 non-null      int64 \n",
      " 13  Question 12     4 non-null      int64 \n",
      " 14  Question 13a    4 non-null      int64 \n",
      " 15  Question 13b    4 non-null      int64 \n",
      " 16  Question 13c    4 non-null      int64 \n",
      " 17  Question 13d    4 non-null      int64 \n",
      " 18  Question 13e    4 non-null      int64 \n",
      " 19  Question 13f    4 non-null      int64 \n",
      " 20  Question 13g    4 non-null      int64 \n",
      " 21  Question 13h    4 non-null      int64 \n",
      " 22  Question 13i    4 non-null      int64 \n",
      " 23  Question Total  4 non-null      int64 \n",
      "dtypes: int64(23), object(1)\n",
      "memory usage: 900.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Identify columns related to T1 scores\n",
    "t1_score_columns = [col for col in data.columns if ('T1_' in col and '_Score' in col) or ('groupID' in col)]\n",
    "t2_score_columns = [col for col in data.columns if ('T2_' in col and '_Score' in col) or ('groupID' in col)]\n",
    "\n",
    "print(t1_score_columns)\n",
    "print(t2_score_columns)\n",
    "\n",
    "t1_correct = data.loc[:,t1_score_columns]\n",
    "t1_correct['moment'] = 'T1'\n",
    "\n",
    "# Remove prefix \"T1_\" and \"T2_\" and suffix \"_Score\" from column names\n",
    "t1_correct.columns = t1_correct.columns.str.replace('T1_', '').str.replace('_Score', '').str.replace('KQ_', 'Question ')\n",
    "\n",
    "t2_correct = data.loc[:,t2_score_columns]\n",
    "t2_correct['moment'] = 'T2'\n",
    "\n",
    "# Remove prefix \"T1_\" and \"T2_\" and suffix \"_Score\" from column names\n",
    "t2_correct.columns = t2_correct.columns.str.replace('T2_', '').str.replace('_Score', '').str.replace('KQ_', 'Question ')\n",
    "\n",
    "# Concatenate t1_correct and t2_correct DataFrames\n",
    "correct = pd.concat([t1_correct, t2_correct])\n",
    "\n",
    "correct\n",
    "\n",
    "correct = correct.groupby(['groupID', 'moment']).agg('sum').reset_index()\n",
    "\n",
    "correct.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>groupID</th>\n",
       "      <th>Question</th>\n",
       "      <th colspan=\"2\" halign=\"left\">0</th>\n",
       "      <th colspan=\"2\" halign=\"left\">1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moment</th>\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Question 1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Question 10</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question 11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Question 12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Question 13a</td>\n",
       "      <td>27.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Question 13b</td>\n",
       "      <td>35.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Question 13c</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Question 13d</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Question 13e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Question 13f</td>\n",
       "      <td>26.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Question 13g</td>\n",
       "      <td>24.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Question 13h</td>\n",
       "      <td>27.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Question 13i</td>\n",
       "      <td>31.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Question 2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Question 3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Question 4</td>\n",
       "      <td>23.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Question 5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Question 6</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Question 7</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Question 8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Question 9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Question Total</td>\n",
       "      <td>364.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>1024.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "groupID        Question      0             1        \n",
       "moment                      T1     T2     T1      T2\n",
       "0            Question 1   16.0   12.0   33.0    26.0\n",
       "1           Question 10   11.0   11.0   30.0    59.0\n",
       "2           Question 11   10.0    8.0    8.0     9.0\n",
       "3           Question 12    8.0   15.0   17.0    19.0\n",
       "4          Question 13a   27.0   23.0   60.0    68.0\n",
       "5          Question 13b   35.0   22.0   68.0    69.0\n",
       "6          Question 13c   16.0   13.0   34.0    44.0\n",
       "7          Question 13d   17.0   13.0   41.0    54.0\n",
       "8          Question 13e   16.0   13.0   23.0    38.0\n",
       "9          Question 13f   26.0   20.0   52.0    67.0\n",
       "10         Question 13g   24.0   15.0   55.0    70.0\n",
       "11         Question 13h   27.0   21.0   60.0    72.0\n",
       "12         Question 13i   31.0   22.0   58.0    70.0\n",
       "13           Question 2    9.0    9.0   19.0    59.0\n",
       "14           Question 3   25.0   17.0   56.0    66.0\n",
       "15           Question 4   23.0   19.0   63.0    71.0\n",
       "16           Question 5    3.0    4.0    9.0    18.0\n",
       "17           Question 6   22.0   11.0   52.0    28.0\n",
       "18           Question 7   23.0   14.0   36.0    63.0\n",
       "19           Question 8    8.0    2.0   19.0    15.0\n",
       "20           Question 9    9.0   10.0   34.0    39.0\n",
       "21       Question Total  364.0  294.0  973.0  1024.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Melt the 'correct' DataFrame\n",
    "melted_correct = correct.melt(id_vars=['groupID', 'moment'], var_name='Question', value_name='Score')\n",
    "\n",
    "# Pivot the melted DataFrame to get the desired format\n",
    "result = melted_correct.pivot_table(index=['Question'], columns=['groupID', 'moment'], values='Score')\n",
    "\n",
    "# Reset the index\n",
    "result.reset_index(inplace=True)\n",
    "\n",
    "# Now 'result' contains the desired DataFrame with Question as rows and GroupID and Moment as separate columns\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22 entries, 0 to 21\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   (Question, )  22 non-null     object \n",
      " 1   (0, T1)       22 non-null     float64\n",
      " 2   (0, T2)       22 non-null     float64\n",
      " 3   (1, T1)       22 non-null     float64\n",
      " 4   (1, T2)       22 non-null     float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 1012.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>groupID</th>\n",
       "      <th>Question</th>\n",
       "      <th colspan=\"2\" halign=\"left\">0</th>\n",
       "      <th colspan=\"2\" halign=\"left\">1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moment</th>\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Question 1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Question 10</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question 11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Question 12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Question 13a</td>\n",
       "      <td>27.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Question 13b</td>\n",
       "      <td>35.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Question 13c</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Question 13d</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Question 13e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Question 13f</td>\n",
       "      <td>26.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Question 13g</td>\n",
       "      <td>24.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Question 13h</td>\n",
       "      <td>27.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Question 13i</td>\n",
       "      <td>31.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Question 2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Question 3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Question 4</td>\n",
       "      <td>23.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Question 5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Question 6</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Question 7</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Question 8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Question 9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Question Total</td>\n",
       "      <td>364.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>1024.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "groupID        Question      0             1        \n",
       "moment                      T1     T2     T1      T2\n",
       "0            Question 1   16.0   12.0   33.0    26.0\n",
       "1           Question 10   11.0   11.0   30.0    59.0\n",
       "2           Question 11   10.0    8.0    8.0     9.0\n",
       "3           Question 12    8.0   15.0   17.0    19.0\n",
       "4          Question 13a   27.0   23.0   60.0    68.0\n",
       "5          Question 13b   35.0   22.0   68.0    69.0\n",
       "6          Question 13c   16.0   13.0   34.0    44.0\n",
       "7          Question 13d   17.0   13.0   41.0    54.0\n",
       "8          Question 13e   16.0   13.0   23.0    38.0\n",
       "9          Question 13f   26.0   20.0   52.0    67.0\n",
       "10         Question 13g   24.0   15.0   55.0    70.0\n",
       "11         Question 13h   27.0   21.0   60.0    72.0\n",
       "12         Question 13i   31.0   22.0   58.0    70.0\n",
       "13           Question 2    9.0    9.0   19.0    59.0\n",
       "14           Question 3   25.0   17.0   56.0    66.0\n",
       "15           Question 4   23.0   19.0   63.0    71.0\n",
       "16           Question 5    3.0    4.0    9.0    18.0\n",
       "17           Question 6   22.0   11.0   52.0    28.0\n",
       "18           Question 7   23.0   14.0   36.0    63.0\n",
       "19           Question 8    8.0    2.0   19.0    15.0\n",
       "20           Question 9    9.0   10.0   34.0    39.0\n",
       "21       Question Total  364.0  294.0  973.0  1024.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.info()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within-group p-value, Treatment (T1 vs. T2): 0.0006937980651855469\n",
      "Within-group p-value, Control (T1 vs. T2): 0.0008734281806367522\n",
      "Between-group p-value at T1: 0.0009256239560594297\n",
      "Between-group p-value at T2: 1.6373263643966353e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import wilcoxon, mannwhitneyu\n",
    "\n",
    "# Assuming `df` is your DataFrame and it's structured as described earlier\n",
    "# Renaming columns for ease of access\n",
    "result.columns = ['Question', 'Control_T1', 'Control_T2', 'Treatment_T1', 'Treatment_T2']\n",
    "result.reset_index()\n",
    "\n",
    "# Within-group comparison: Treatment group, T1 vs. T2 (to assess intervention effect)\n",
    "stat, p_within_treatment = wilcoxon(result['Treatment_T1'], result['Treatment_T2'])\n",
    "\n",
    "# Within-group comparison: Control group, T1 vs. T2 (to assess intervention effect)\n",
    "stat, p_within_control = wilcoxon(result['Control_T1'], result['Control_T2'])\n",
    "\n",
    "# Between-group comparison at T1 (baseline)\n",
    "stat, p_between_t1 = mannwhitneyu(result['Control_T1'], result['Treatment_T1'])\n",
    "\n",
    "# Between-group comparison at T2 (post-intervention)\n",
    "stat, p_between_t2 = mannwhitneyu(result['Control_T2'], result['Treatment_T2'])\n",
    "\n",
    "# Output the calculated p-values\n",
    "print(f\"Within-group p-value, Treatment (T1 vs. T2): {p_within_treatment}\")\n",
    "print(f\"Within-group p-value, Control (T1 vs. T2): {p_within_control}\")\n",
    "print(f\"Between-group p-value at T1: {p_between_t1}\")\n",
    "print(f\"Between-group p-value at T2: {p_between_t2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Control_T1</th>\n",
       "      <th>Control_T2</th>\n",
       "      <th>Treatment_T1</th>\n",
       "      <th>Treatment_T2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Question 1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Question 10</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question 11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Question 12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Question 13a</td>\n",
       "      <td>27.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Question 13b</td>\n",
       "      <td>35.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Question 13c</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Question 13d</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Question 13e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Question 13f</td>\n",
       "      <td>26.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Question 13g</td>\n",
       "      <td>24.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Question 13h</td>\n",
       "      <td>27.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Question 13i</td>\n",
       "      <td>31.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Question 2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Question 3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Question 4</td>\n",
       "      <td>23.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Question 5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Question 6</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Question 7</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Question 8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Question 9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Question Total</td>\n",
       "      <td>364.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>1024.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Question  Control_T1  Control_T2  Treatment_T1  Treatment_T2\n",
       "0       Question 1        16.0        12.0          33.0          26.0\n",
       "1      Question 10        11.0        11.0          30.0          59.0\n",
       "2      Question 11        10.0         8.0           8.0           9.0\n",
       "3      Question 12         8.0        15.0          17.0          19.0\n",
       "4     Question 13a        27.0        23.0          60.0          68.0\n",
       "5     Question 13b        35.0        22.0          68.0          69.0\n",
       "6     Question 13c        16.0        13.0          34.0          44.0\n",
       "7     Question 13d        17.0        13.0          41.0          54.0\n",
       "8     Question 13e        16.0        13.0          23.0          38.0\n",
       "9     Question 13f        26.0        20.0          52.0          67.0\n",
       "10    Question 13g        24.0        15.0          55.0          70.0\n",
       "11    Question 13h        27.0        21.0          60.0          72.0\n",
       "12    Question 13i        31.0        22.0          58.0          70.0\n",
       "13      Question 2         9.0         9.0          19.0          59.0\n",
       "14      Question 3        25.0        17.0          56.0          66.0\n",
       "15      Question 4        23.0        19.0          63.0          71.0\n",
       "16      Question 5         3.0         4.0           9.0          18.0\n",
       "17      Question 6        22.0        11.0          52.0          28.0\n",
       "18      Question 7        23.0        14.0          36.0          63.0\n",
       "19      Question 8         8.0         2.0          19.0          15.0\n",
       "20      Question 9         9.0        10.0          34.0          39.0\n",
       "21  Question Total       364.0       294.0         973.0        1024.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('/Users/nagesh/Library/CloudStorage/OneDrive-NextGenMetricsB.V/Personal/Anisha/M3/results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
